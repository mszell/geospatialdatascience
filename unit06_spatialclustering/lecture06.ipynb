{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"frontmatter text-center\">\n",
    "<h1>Geospatial Data Science</h1>\n",
    "<h2>Lecture 6: Spatial clustering</h2>\n",
    "<h3>IT University of Copenhagen, Spring 2023</h3>\n",
    "<h3>Instructor: Ane Rahbek Vierø</h3>\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook was adapted from:\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bG/lab_G.html\n",
    "\n",
    "# Clustering, spatial clustering, and geodemographics\n",
    "\n",
    "<details>\n",
    "<summary>Background</summary>\n",
    "\n",
    "This session covers statistical clustering of spatial observations. Many questions and topics are complex phenomena that involve several dimensions and are hard to summarize into a single variable. In statistical terms, we call this family of problems *multivariate*, as opposed to *univariate* cases where only a single variable is considered in the analysis. Clustering tackles this kind of questions by reducing their dimensionality -the number of relevant variables the analyst needs to look at- and converting it into a more intuitive set of classes that even non-technical audiences can look at and make sense of. For this reason, it is widely use in applied contexts such as policymaking or marketting. In addition, since these methods do not require many preliminary assumptions about the structure of the data, it is a commonly used exploratory tool, as it can quickly give clues about the shape, form and content of a dataset.\n",
    "\n",
    "The basic idea of statistical clustering is to summarize the information contained in several variables by creating a relatively small number of categories. Each observation in the dataset is then assigned to one, and only one, category depending on its values for the variables originally considered in the classification. If done correctly, the exercise reduces the complexity of a multi-dimensional problem while retaining all the meaningful information contained in the original dataset. This is because, once classified, the analyst only needs to look at in which category every observation falls into, instead of considering the multiple values associated with each of the variables and trying to figure out how to put them together in a coherent sense. When the clustering is performed on observations that represent areas, the technique is often called geodemographic analysis.\n",
    "\n",
    "Although there exist many techniques to statistically group observations in a dataset, all of them are based on the premise of using a set of attributes to define classes or categories of observations that are similar *within* each of them, but differ *between* groups. How similarity within groups and dissimilarity between them is defined and how the classification algorithm is operationalized is what makes techniques differ and also what makes each of them particularly well suited for specific problems or types of data. As an illustration, we will only dip our toes into one of these methods, K-means, which is probably the most commonly used technique for statistical clustering.\n",
    "\n",
    "In the case of analysing spatial data, there is a subset of methods that are of particular interest for many common cases in Geographic Data Science. These are the so-called *regionalization* techniques. Regionalization methods can take also many forms and faces but, at their core, they all involve statistical clustering of observations with the additional constraint that observations need to be geographical neighbors to be in the same category. Because of this, rather than category, we will use the term *area* for each observation and *region* for each category, hence regionalization, the construction of regions from smaller areas.\n",
    "\n",
    "</details>\n",
    "\n",
    "### New library: sklearn\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/index.html) is the core library for machine learning in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import contextily as cx \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import esda\n",
    "from splot.libpysal import plot_spatial_weights\n",
    "from pysal.lib import weights\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This weeks lecture data is a preprocessed data set with demographic and socio-economic variables for each voting area in Denmark. The original data were downloaded from [Statistics Denmark](https://valgdatabase.dst.dk/).\n",
    "*Note that there are a few missing data points, which will show up as blank areas on our map.*\n",
    "\n",
    "For this lecture we will only look at the Greater Copenhagen area and on variables for the **share of households** in different **income brackets**, household **car ownership**, share of **immigrants**, and **population density**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio = pd.read_csv('data/socio_economic_data.csv')\n",
    "areas = gpd.read_file('data/voting_areas.gpkg')\n",
    "\n",
    "# Just like previous lectures/exercises, we need to join the variables and the geometries\n",
    "socio_areas = areas.merge(socio,left_on='ValgstedId',right_on='Gruppe',how='inner')\n",
    "\n",
    "socio_areas.set_index('Gruppe',inplace=True,drop=False)\n",
    "\n",
    "rename_cols = {\n",
    "       'households_share_income_under100k':'income_under100k',\n",
    "       'households_share_income_100-149.9k':'income_100-149.9k',\n",
    "       'households_share_income_150-199.9k':'income_150-199.9k',\n",
    "       'households_share_income_200-299.9k':'income_200-299.9k',\n",
    "       'households_share_income_300-399.9k':'income_300-399.9k',\n",
    "       'households_share_income_400-499.9k':'income_400-499.9k',\n",
    "       'households_share_income_500-749.9k':'income_500-749.9k', \n",
    "       'households_share_income_750-k':'income_750+k',\n",
    "       'share_households_1car':'1_car',\n",
    "       'share_households_2pluscar':'2+cars', \n",
    "       'share_households_nocar':'no_car'\n",
    "}\n",
    "\n",
    "# Rename columns for nicer plot titles etc. later\n",
    "socio_areas.rename(rename_cols,axis=1,inplace=True)\n",
    "\n",
    "socio_areas['low_income'] = socio_areas[\"income_under100k\"] + socio_areas['income_100-149.9k'] + socio_areas[\"income_150-199.9k\"]\n",
    "socio_areas['medium_income'] = socio_areas[\"income_200-299.9k\"] + socio_areas[\"income_300-399.9k\"] + socio_areas[\"income_400-499.9k\"]\n",
    "socio_areas['high_income'] = socio_areas[\"income_500-749.9k\"] + socio_areas[\"income_750+k\"]\n",
    "\n",
    "immi_cols = [\n",
    "'Nordiske lande',\n",
    "'Tyrkiet', \n",
    "'Tidligere Jugoslavien', \n",
    "'Gamle EU-lande', \n",
    "'Nye EU-lande',\n",
    "'Øvrige Europa', \n",
    "'Afrika', \n",
    "'Nordamerika', \n",
    "'Syd- og Mellemamerika',\n",
    "'Asien og Oceanien', \n",
    "'Uoplyst']\n",
    "\n",
    "socio_areas['immigration'] = socio_areas[immi_cols].sum(axis=1) / socio_areas.total_population\n",
    "\n",
    "socio_areas['pop_density'] = socio_areas.total_population / (socio_areas.area/1000000)\n",
    "\n",
    "keep_cols = [\n",
    "       'Gruppe',\n",
    "       'municipal_id', \n",
    "       'area_name', \n",
    "       'geometry', \n",
    "       'total_population',\n",
    "       'pop_density',\n",
    "       'low_income',\n",
    "       'medium_income',\n",
    "       'high_income',\n",
    "       'immigration',\n",
    "       'total_households', \n",
    "       '1_car',\n",
    "       '2+cars', \n",
    "       'no_car'\n",
    "]\n",
    "\n",
    "socio_areas = socio_areas[keep_cols]\n",
    "\n",
    "muni_codes = [\n",
    "       '0101',\n",
    "       '0147',\n",
    "       '0155',\n",
    "       '0185',\n",
    "       '0165',\n",
    "       '0151',\n",
    "       '0153',\n",
    "       '0157',\n",
    "       '0159',\n",
    "       '0161',\n",
    "       '0163',\n",
    "       '0167',\n",
    "       '0169',\n",
    "       '0183',\n",
    "       '0173',\n",
    "       '0175',\n",
    "       '0187',\n",
    "       '0201',\n",
    "       '0240',\n",
    "       '0210',\n",
    "       '0250',\n",
    "       '0190',\n",
    "       '0270',\n",
    "       '0260',\n",
    "       '0217',\n",
    "       '0219',\n",
    "       '0223',\n",
    "       '0230'\n",
    "]\n",
    "\n",
    "data = socio_areas.loc[socio_areas.municipal_id.isin(muni_codes)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of the table\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data.plot(linewidth=0.3);\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in this section we will also need the geometries for the municipalities. We can create the municipal geometries by **dissolving** voting areas with the same municipal id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni = areas[['geometry','municipal_id']].dissolve('municipal_id')\n",
    "\n",
    "muni = muni.loc[muni_codes]\n",
    "\n",
    "ax = muni.plot(linewidth=0.3);\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into exploring the data, one additional step that will come in handy down the line. Not every variable in the table is an attribute that we will want for the clustering. \n",
    "\n",
    "We are interested in income, population density, share of immigrants, and car ownership column, so we will only consider those. Let us create a list with the names of all columns that we will use for the clustering, so they are easier to subset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = [ \n",
    "'pop_density',\n",
    "'immigration',\n",
    "'low_income', \n",
    "'medium_income', \n",
    "'high_income', \n",
    "'1_car', \n",
    "'2+cars', \n",
    "'no_car'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know the data\n",
    "\n",
    "\n",
    "The best way to start exploring the geography of income etc. is by plotting each of them onto a different map. This will give us a **univariate** perspective on each of the variables we are interested in.\n",
    "\n",
    "Since we have many columns to plot, we will create a loop that generates each map for us and places it on a \"subplot\" of the main figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes (this time it's 9, arranged 3 by 3)\n",
    "f, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 20))\n",
    "# Make the axes accessible with single indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Start the loop over all the variables of interest\n",
    "for i, col in enumerate(cluster_cols):\n",
    "    # select the axis where the map will go\n",
    "    ax = axs[i]\n",
    "    # Plot the map\n",
    "    data.plot(\n",
    "        column=col, \n",
    "        ax=ax, \n",
    "        scheme='Quantiles',\n",
    "        linewidth=0.2, \n",
    "        cmap='cividis', \n",
    "        alpha=0.75,\n",
    "    )\n",
    "    # Remove axis clutter\n",
    "    ax.set_axis_off()\n",
    "    # Set the axis title to the name of variable being plotted\n",
    "    ax.set_title(col)\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary><b>Walk through of plotting method</b></summary>\n",
    "\n",
    "Let us walk through the process of creating the figure above, which involves several subplots inside the same figure:\n",
    "\n",
    "* First we set the number of rows and columns we want for the grid of subplots.\n",
    "* The resulting object, `axs`, is not a single one but a grid (or array) of axis. Because of this, we can't plot directly on `axs`, but instead we need access each individual axis.\n",
    "* To make that step easier, we *unpack* the grid into a flat list (array) for the axes of each subplot with `flatten` \n",
    "* At this point, we set up a `for` loop to plot a map in each of the subplots.\n",
    "* Within the loop, we extract the axis, plot the choropleth on it and style the map.\n",
    "* Display the figure.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "As we can see, there is substantial variation in how the variable values are distributed over space. There seems to be a clear urban-rural divide in the pattern for car ownership, but also the household income appear to follow some spatial pattern.\n",
    "\n",
    "Let's check statistically what we suspect when looking at the plots by eye - that there is a high level of significant clustering for all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights.KNN.from_dataframe(data,k=8)\n",
    "# Calculate Moran's I for each variable\n",
    "mi_results = [esda.Moran(data[variable], w) for variable in cluster_cols]\n",
    "# Structure results as a list of tuples\n",
    "mi_results = [(variable, res.I, res.p_sim) for variable,res in zip(cluster_cols, mi_results)]\n",
    "# Display on table\n",
    "table = pd.DataFrame(\n",
    "    mi_results,\n",
    "    columns=['Variable', \"Moran's I\", 'P-value']\n",
    ").set_index('Variable')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A multivariate look\n",
    "\n",
    "Even though we only have eleven variables, it is very hard to \"mentally overlay\" all of them to come up with an overall assessment of the nature of each part of the area. For pairwise bivariate correlations, a useful tool is the correlation matrix plot, available in `seaborn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[cluster_cols], kind='reg', diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is helpful to consider uni and bivariate questions such as: *what is the relationship between high income and car ownership?* (Positive) *Are their areas with both a high share of high and low income households?* (Not really). However, sometimes, this is not enough and we are interested in more sophisticated questions that are truly multivariate and, in these cases, the figure above cannot help us. For example, it is not straightforward to answer questions like: *Are all areas with low car ownership similar?* *Are the Northen and Western suburbs similar?* For these kinds of multi-dimensional questions -involving multiple variables at the same time- we require a truly multidimensional method like statistical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An geodemographic classification of Greater Copenhagen using K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Geodemographic analysis</b></summary>\n",
    "\n",
    "A geodemographic analysis involves the classification of the areas that make up a geographical map into groups or categories of observations that are similar within each other but different between them. The classification is carried out using a statistical clustering algorithm that takes as input a set of attributes and returns the group (\"labels\" in the terminology) each observation belongs to. Depending on the particular algorithm employed, additional parameters, such as the desired number of clusters employed or more advanced tuning parameters (e.g. bandwith, radius, etc.), also need to be entered as inputs. For our geodemographic classification of demographic and socio-economic variables in Greater Cph, we will use one of the most popular clustering algorithms: K-means. This technique only requires as input the observation attributes and the final number of groups that we want it to cluster the observations into. In our case, we will usesix to begin with as this will allow us to have a closer look into each of them.\n",
    "\n",
    "</details>\n",
    "\n",
    "Although the underlying algorithm is not trivial, running K-means in Python is streamlined thanks to `scikit-learn`. Similar to the extensive set of available algorithms in the library, its computation is a matter of two lines of code. First, we need to specify the parameters in the `KMeans` method (which is part of `scikit-learn`'s `cluster` submodule). Note that, at this point, we do not even need to pass the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans6 = cluster.KMeans(n_clusters=6, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up an object that holds all the parameters required to run the algorithm. In our case, we only passed the number of clusters(`n_clusters`) and the random state, a number that ensures every run of K-Means, which relies on random initialisations, is the same and thus reproducible.\n",
    "\n",
    "To actually run the algorithm on the attributes, we need to call the `fit` method in `kmeans5`.\n",
    "First, we make sure to standardize our data using the `robust_scaling` method from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cluster_cols:\n",
    "    data[c+'_scaled'] = robust_scale(data[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols_scaled = [c+'_scaled' for c in cluster_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the clustering algorithm\n",
    "k6cls = kmeans6.fit(data[cluster_cols_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `k6cls` object we have just created contains several components that can be useful for an analysis. For now, we will use the labels, which represent the different categories in which we have grouped the data. Remember, in Python, life starts at zero, so the group labels go from zero to five. Labels can be  extracted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k6cls.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number represents a different category, so two observations with the same number belong to same group. The labels are returned in the same order as the input attributes were passed in, which means we can append them to the original table of data as an additional column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['k6cls'] = k6cls.labels_\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the categories\n",
    "\n",
    "To get a better understanding of the classification we have just performed, it is useful to display the categories created on a map. For this, we will use a unique values choropleth, which will automatically assign a different color to each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "data.plot(\n",
    "    column='k6cls', \n",
    "    categorical=True, \n",
    "    legend=True, \n",
    "    linewidth=0.2, \n",
    "    ax=ax, \n",
    "    cmap='tab20b'\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add title\n",
    "plt.title('Socio-Economic Classification of Greater Cph')\n",
    "\n",
    "# Add basemap\n",
    "cx.add_basemap(\n",
    "    ax,\n",
    "    crs=muni.crs,\n",
    "    source=cx.providers.CartoDB.DarkMatterNoLabels\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map above represents the geographical distribution of the six categories created by the K-means algorithm. A quick glance shows a strong spatial structure in the distribution of the colors: even though the algorithm did not include the location of values, several spatially connected clusters appear.\n",
    "\n",
    "### Exploring the nature of the categories\n",
    "\n",
    "Once we have a sense of where and how the categories are distributed over space, it is also useful to explore them statistically. This will allow us to characterize them, giving us an idea of the kind of observations subsumed into each of them. As a first step, let us find how many observations are in each category. To do that, we will make use of the `groupby` operator, combined with the function `size`, which returns the number of elements in a subgroup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k6sizes = data.groupby('k6cls').size()\n",
    "k6sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby` operator groups a table (`DataFrame`) using the values in the column provided (`k6cls`) and passes them onto the function provided afterwards, which in this case is `size`. Effectively, what this does is to groupby the observations by the categories created and count how many of them each contains. For a more visual representation of the output, a bar plot is a good alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k6sizes.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected from the map, the clusters have very different sizes.\n",
    "\n",
    "In order to describe the nature of each category, we can look at the values of each of the attributes we have used to create them in the first place, for example by checking the average value of each. To do that in Python, we will rely on the `groupby` operator which we will combine it with the function `mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean by group\n",
    "k6means = data.groupby('k6cls')[cluster_cols].mean()\n",
    "# Show the table transposed (so it's not too wide)\n",
    "k6means.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way of constructing cluster profiles is to draw the distributions of cluster members’ data. To do this we need to “tidy up” the dataset. A tidy dataset is one where every row is an observation, and every column is a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index db on cluster ID\n",
    "tidy_db = data.set_index('k6cls')\n",
    "# Keep only variables used for clustering\n",
    "tidy_db = tidy_db[cluster_cols]\n",
    "# Stack column names into a column, obtaining\n",
    "# a \"long\" version of the dataset\n",
    "tidy_db = tidy_db.stack()\n",
    "# Take indices into proper columns\n",
    "tidy_db = tidy_db.reset_index()\n",
    "# Rename column names\n",
    "tidy_db = tidy_db.rename(\n",
    "    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n",
    ")\n",
    "# Check out result\n",
    "tidy_db.head(30)\n",
    "\n",
    "\n",
    "# Scale fonts to make them more readable\n",
    "sns.set(font_scale=1.5)\n",
    "# Setup the facets\n",
    "facets = sns.FacetGrid(\n",
    "    data=tidy_db,\n",
    "    col=\"Attribute\",\n",
    "    hue='k6cls',\n",
    "    sharey=False,\n",
    "    sharex=False,\n",
    "    aspect=2,\n",
    "    col_wrap=3,\n",
    ")\n",
    "# Build the plot from `sns.kdeplot`\n",
    "facets.map(sns.kdeplot, \"Values\", fill=True,warn_singular=False).add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the section on geodemographics. As we have seen, the essence of this approach is to group areas based on a purely statistical basis: *where* each area is located is irrelevant for the label it receives from the clustering algorithm. In many contexts, this is not only permissible but even desirable, as the interest is to see if particular combinations of values are distributed over space in any discernible way. However, in other context, we may be interested in created groups of observations that follow certain spatial constraints. For that, we now turn into regionalization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regionalization algorithms\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Regionalization background</summary>\n",
    "\n",
    "Regionalization is the subset of clustering techniques that impose a spatial constraint on the classification. In other words, the result of a regionalization algorithm contains areas that are spatially contiguous. Effectively, what this means is that these techniques aggregate areas into a smaller set of larger ones, called regions. In this context then, areas are *nested* within regions. Real world examples of this phenomenon includes counties within states or, in the UK, local super output areas (LSOAs) into middle super output areas (MSOAs). The difference between those examples and the output of a regionalization algorithm is that while the former are aggregated based on administrative principles, the latter follows a statistical technique that, very much the same as in the standard statistical clustering, groups together areas that are similar on the basis of a set of attributes. Only that now, such statistical clustering is spatially constrained.\n",
    "\n",
    "As in the non-spatial case, there are many different algorithms to perform regionalization, and they all differ on details relating to the way they measure (dis)similarity, the process to regionalize, etc. However, same as above too, they all share a few common aspects. In particular, they all take a set of input attributes *and* a representation of space in the form of a binary spatial weights matrix. Depending on the algorithm, they also require the desired number of output regions into which the areas are aggregated.\n",
    "\n",
    "To illustrate these concepts, we will run a regionalization algorithm on the socio-economic data we have been using. In this case, the goal will be to re-delineate the boundary lines following a rationale based on the different average values, instead of the administrative reasons behind the existing boundary lines. In this way, the resulting regions will represent a consistent set of areas that are similar with each other in terms of income etc.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining space formally\n",
    "\n",
    "Very much in the same way as with ESDA techniques, regionalization methods require a formal representation of space that is statistics-friendly. In practice, this means that we will need to create a spatial weights matrix for the areas to be aggregated.\n",
    "\n",
    "Because of the (geographical) islands in our data set we use a K-nearest-neighbor methods, which technically does not guarantee that neighbors are spatially connected.\n",
    "Keeping the value of K low will however make sure that most neighbors are actually adjacent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights.KNN.from_dataframe(data,k=4,ids='Gruppe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spatial_weights(w, data,indexed_on='Gruppe');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating regions from areas\n",
    "\n",
    "At this point, we have all the pieces needed to run a regionalization algorithm. For this example, we will use a spatially-constrained version of the agglomerative algorithm. This is a similar approach to that used above (the inner-workings of the algorithm are different however) with the difference that, in this case, observations can only be labelled in the same group if they are spatial neighbors, as defined by our spatial weights matrix `w`. The way to interact with the algorithm is very similar to that above. We first set the parameters:\n",
    "\n",
    "*Note that you usually will want to create more clusters than when running e.g. K-Means with no spatial constraint, since similar but disconnected areas will create each their cluster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagg13 = cluster.AgglomerativeClustering(n_clusters=12, connectivity=w.sparse)\n",
    "sagg13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can run the algorithm by calling `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the clustering algorithm - again using the scaled data\n",
    "sagg13cls = sagg13.fit(data[cluster_cols_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we append the labels to the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sagg13cls'] = sagg13cls.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean by group\n",
    "sagg13_data = data.groupby('sagg13cls')[cluster_cols].mean()\n",
    "# Show the table transposed (so it's not too wide)\n",
    "sagg13_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index db on cluster ID\n",
    "tidy_db = data.set_index('sagg13cls')\n",
    "# Keep only variables used for clustering\n",
    "tidy_db = tidy_db[cluster_cols]\n",
    "# Stack column names into a column, obtaining\n",
    "# a \"long\" version of the dataset\n",
    "tidy_db = tidy_db.stack()\n",
    "# Take indices into proper columns\n",
    "tidy_db = tidy_db.reset_index()\n",
    "# Rename column names\n",
    "tidy_db = tidy_db.rename(\n",
    "    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n",
    ")\n",
    "# Check out result\n",
    "tidy_db.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom colormap to ensure uniform colors between plots and maps\n",
    "colors = [\"#a6cee3\",\n",
    "\"#1f78b4\",\n",
    "\"#b2df8a\",\n",
    "\"#33a02c\",\n",
    "\"#fb9a99\",\n",
    "\"#e31a1c\",\n",
    "\"#fdbf6f\",\n",
    "\"#ff7f00\",\n",
    "\"#cab2d6\",\n",
    "\"#6a3d9a\",\n",
    "\"#ffff99\",\n",
    "\"#b15928\"]\n",
    "\n",
    "keys = list(range(0,12))\n",
    "\n",
    "color_dict={}\n",
    "\n",
    "for k, c in zip(keys, colors):\n",
    "    color_dict[k] = c\n",
    "\n",
    "vals = data.sagg13cls.unique()\n",
    "vals.sort()\n",
    "cmap = mpl.colors.ListedColormap([color_dict[b] for b in vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale fonts to make them more readable\n",
    "sns.set(font_scale=1.5)\n",
    "# Setup the facets\n",
    "facets = sns.FacetGrid(\n",
    "    data=tidy_db,\n",
    "    col=\"Attribute\",\n",
    "    hue='sagg13cls',\n",
    "    sharey=False,\n",
    "    sharex=False,\n",
    "    aspect=2,\n",
    "    palette= colors, #'tab20',\n",
    "    col_wrap=3,\n",
    ")\n",
    "# Build the plot from `sns.kdeplot`\n",
    "facets.map(sns.kdeplot, \"Values\", fill=True, warn_singular=False).add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS:** Notice that clusters/regions with only one member are not plotted in the density plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the resulting regions\n",
    "\n",
    "At this point, the column `sagg13cls` is no different than `k6cls`: a categorical variable that can be mapped into a unique values choropleth. In fact the following code snippett is exactly the same as before, only replacing the name of the variable to be mapped and the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(10,10))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "data.plot(\n",
    "    column='sagg13cls', categorical=True, legend=True, linewidth=0, cmap=cmap, ax=ax\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add title\n",
    "plt.title('Socio-economic regions for Greater Cph')\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing organic and administrative delineations\n",
    "\n",
    "The map above gives a very clear impression of the boundary delineation of the algorithm. However, it is still based on the small area polygons. To create the new boroughs \"properly\", we need to dissolve all the polygons in each category into a single one. This is a standard GIS operation that is supported by `geopandas`. The only additional complication is that we need to wrap it into a separate function to be able to pass it on to `groupby`. We first the define the function `dissolve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissolve(gs):\n",
    "    '''\n",
    "    Take a series of polygons and dissolve them into a single one\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    gs        : GeoSeries\n",
    "                Sequence of polygons to be dissolved\n",
    "    Returns\n",
    "    -------\n",
    "    dissolved : Polygon\n",
    "                Single polygon containing all the polygons in `gs`\n",
    "    '''\n",
    "    return gs.unary_union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundaries for the clustering regions can then be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve the polygons based on `sagg13cls`\n",
    "data_regions = gpd.GeoDataFrame(geometry=\n",
    "    data.groupby(data['sagg13cls']).apply(dissolve),\n",
    "    crs=data.crs\n",
    ")\n",
    "data_regions['cluster'] = data_regions.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Plot boundary lines\n",
    "data_regions.plot(\n",
    "    ax=ax, \n",
    "    linewidth=0.5,\n",
    "    facecolor='white', \n",
    "    edgecolor='k'\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add title\n",
    "plt.title('Socio-economic regions for Greater Cph');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The delineation above provides a view into the geography of socio-economic regions in and around Copenhagen (based on the considered variables!). Each region delineated contains areas that, according to our regionalisation algorithm, are more similar with each other than those in the neighboring areas. Now let's compare this geography that we have organically drawn from our data with that of the official set of administrative boundaries. For example, with the municipalities.\n",
    "\n",
    "Remember we created these at the beginning of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muni.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And displayed in a similar way as with the newly created ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Plot boundary lines\n",
    "muni.plot(\n",
    "    ax=ax, \n",
    "    linewidth=0.5,\n",
    "    edgecolor='k', \n",
    "    facecolor='white'\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add title\n",
    "plt.title('Municipalities in Greater Cph');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to more easily compare the administrative and the \"regionalized\" boundary lines, we can overlay them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "f, ax = plt.subplots(1, figsize=(12, 12))\n",
    "f.set_facecolor(\"k\")\n",
    "\n",
    "# Add regionalisation geography\n",
    "data_regions.plot(\n",
    "    ax=ax,\n",
    "    column='cluster',\n",
    "    categorical=True,\n",
    "    cmap='tab20b',\n",
    "    legend=True\n",
    ")\n",
    "\n",
    "# Add municipalities\n",
    "muni.plot(\n",
    "    ax=ax,\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"xkcd:blue\",\n",
    "    linewidth=1,\n",
    "    linestyle='dashed'\n",
    ")\n",
    "\n",
    "# Add basemap\n",
    "cx.add_basemap(\n",
    "    ax,\n",
    "    crs=muni.crs,\n",
    "    source=cx.providers.CartoDB.DarkMatterNoLabels\n",
    ")\n",
    "\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display clean\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the figure, we can see that several or the more rural municipalities have ended up completely covered by one cluster/region, while the more urban one on the other hand shows considerably more variation. It is also evident that some of the clusters span across municipal divides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_regions.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "09273b9d70777594a7b7f3f6c3e8e91be451f5d194695438b5ebcc24128e9f82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
